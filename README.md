# üß† Sistema de Generaci√≥n Aumentada por Recuperaci√≥n (RAG) con LLaMA 3.2 como asistente para consultas

Este repositorio contiene un sistema completo para la integraci√≥n local de un modelo **LLaMA 3.2** (versi√≥n 3B) en un entorno **RAG (Retrieval-Augmented Generation)**. El objetivo es permitir la b√∫squeda y consulta sobre documentos locales mediante IA generativa y recuperaci√≥n contextual.

---

## üìÅ Estructura del Proyecto (componentes esenciales)

```
üì¶ main_dir/
‚îú‚îÄ‚îÄ setup_venv_windows.txt   # Instrucciones para configurar el entorno virtual en Windows
‚îú‚îÄ‚îÄ requirements.txt         # Dependencias necesarias para el entorno virtual
‚îú‚îÄ‚îÄ pdfdata/                 # Carpeta con PDFs originales
‚îú‚îÄ‚îÄ txtdata/                 # Carpeta con texto plano (postprocesado)
‚îú‚îÄ‚îÄ data_extractor.py        # Extracci√≥n de texto desde PDF a TXT
‚îú‚îÄ‚îÄ vectorizer.py            # Vectorizaci√≥n, embeddings y base de datos FAISS
‚îú‚îÄ‚îÄ local_client.py          # Cliente con GUI para consultas al servidor
‚îú‚îÄ‚îÄ local_server.py          # Servidor Flask que aloja la instancia LLaMA 3.2 como LLM para las respuestas
‚îú‚îÄ‚îÄ vector_db/               # Base de datos vectorial generada con FAISS
‚îî‚îÄ‚îÄ README.md                # Archivo README del proyecto
```

---

## ‚öôÔ∏è Requisitos previos

Antes de comenzar, aseg√∫rese de cumplir con lo siguiente:

- Sistema operativo: **Windows 10/11** (con soporte para Python).
- **Python 3.10 o 3.11** instalado y a√±adido al PATH.
- Tener `pip` actualizado:  
  ```bash
  python -m pip install --upgrade pip
  ```
- Acceso local a **LLaMA 3.2** (previamente autorizado por Meta y descargado v√≠a Hugging Face).
- Uso de una terminal (preferiblemente PowerShell)

---

## üì¶ `requirements.txt`

El archivo incluye dependencias esenciales como:

```txt
flask                        
faiss-cpu        
langchain           
langchain-community        
langchain-huggingface     
transformers                  
sentence-transformers         
torch                           
numpy                   
PyMuPDF                         
requests                        
accelerate                      
googletrans
```

---

## üêç Creaci√≥n del entorno virtual (Windows)

Siga estos pasos para configurar el entorno virtual en Windows:

```plaintext
# setup_venv_windows.txt

1. Abra la terminal de PowerShell.

2. Navegue al directorio del proyecto:
   cd ruta\al\directorio\del\proyecto

3. Cree un entorno virtual con:
   python -m venv venv

4. Active el entorno virtual mediante:
   .\venv\Scripts\activate

5. Instale todas las dependencias necesarias usando:
   pip install -r requirements.txt

```

Este proceso construye un entorno limpio y aislado, evitando conflictos con otras instalaciones de Python.

---

## üöÄ Ejecuci√≥n del sistema

### 1. Extraer y vectorizar documentos

En primer lugar, se extrae el texto de los archivos PDF del directorio pdfdata y guarda el contenido en formato .txt en el directorio txtdata.

```bash
python data_extractor.py
```

En segundo lugar, se divide los textos en chunks, genera vectores para cada chunk utilizando un modelo de embeddings y almacena los vectores en una base de datos vectorial (FAISS).

```bash
python vectorizer.py
```

Esto gestionar√° `txtdata/`, construyendo los objetos 'document', los vectores sem√°nticos y los √≠ndices document-vector en `vector_db/`.

---

### 2. Levantar el servidor Flask con LLaMA 3.2

Se ejecuta el servidor Flask para responder a las consultas del cliente.

```bash
python llama_server_local.py
```

Esto iniciar√° el backend en `http://192.168.XX.XX:5666/`, que espera consultas del cliente para responder con texto generado por el modelo de LLM: LLaMA 3.2 3B.

---

### 3. Ejecutar la interfaz gr√°fica del cliente

Manteniendo el servidor encendido, se ejecuta el script cliente:

```bash
python llama_client_local.py
```

Se abrir√° una ventana gr√°fica que permite introducir preguntas. El sistema recuperar√° contexto relevante y generar√° respuestas usando el modelo.
Este programa utiliza la base de datos FAISS construida anteriormente para realizar recuperaci√≥n de contexto a la hora de enviar el prompt para contestar las queries. 

---

## üß† Resumen del funcionamiento de este sistema RAG

1. **Extracci√≥n**: El texto se extrae de PDFs y se limpia.
2. **Vectorizaci√≥n**: Se genera una base de vectores sem√°nticos del texto.
3. **Recuperaci√≥n**: Ante una consulta, se buscan los vectores m√°s cercanos en FAISS.
4. **Generaci√≥n**: Se construye un prompt con el contexto recuperado, se solicita la consulta y se genera una respuesta utilizando el modelo LLaMA 3.2.

---

## ü§ù Justificaci√≥n del proyecto

Este proyecto es aprovechado para un **Trabajo de Fin de Grado (TFG)** en Ingenier√≠a Telem√°tica ‚Äì Universidad de Alcal√°.  

---

## üìÑ Licencia

Este proyecto est√° sujeto a las licencias de uso personal y acad√©mico del modelo LLaMA de Meta AI.  
No redistribuya el modelo ni lo uses con fines comerciales sin autorizaci√≥n.

---

**Autor**: H√©ctor N√∫√±ez Calero.

**A√±o**: 2025/2026.

**Contacto**: *[hector.nunez@edu.uah.es]*.
