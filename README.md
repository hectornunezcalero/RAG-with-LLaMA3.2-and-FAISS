# ğŸ§  LLaMA 3.2 RAG System â€“ Local AI Search with Contextual Retrieval

Este repositorio contiene un sistema completo para la integraciÃ³n local de un modelo **LLaMA 3.2** (versiÃ³n 3B) en un entorno **RAG (Retrieval-Augmented Generation)**. El objetivo es permitir la bÃºsqueda y consulta sobre documentos locales mediante IA generativa y recuperaciÃ³n contextual.

---

## ğŸ“ Estructura del Proyecto

```
ğŸ“¦ main_dir/
â”œâ”€â”€ llama_client_local.py    # Cliente para consultas y sesiones (Tkinter + requests)
â”œâ”€â”€ llama_server_local.py    # Servidor Flask que aloja LLaMA 3.2 y gestiona generaciÃ³n de texto
â”œâ”€â”€ vectorizer.py            # VectorizaciÃ³n, embeddings y base de datos FAISS
â”œâ”€â”€ data_extractor.py        # ExtracciÃ³n de texto desde PDF/TXT y preparaciÃ³n de corpus
â”œâ”€â”€ pdfdata/                 # Carpeta con PDFs originales
â”œâ”€â”€ txtdata/                 # Carpeta con texto plano (postprocesado)
â”œâ”€â”€ vector_db/               # Base de datos vectorial generada con FAISS
â”œâ”€â”€ requirements.txt         # Dependencias necesarias para el entorno virtual
â”œâ”€â”€ setup_venv_windows.txt   # Instrucciones para configurar el entorno virtual en Windows
â””â”€â”€ README2.md               # Archivo README del proyecto
```

---

## âš™ï¸ Requisitos previos

Antes de comenzar, asegÃºrate de cumplir con lo siguiente:

- Sistema operativo: **Windows 10/11** (con soporte para Python).
- **Python 3.10 o 3.11** instalado y aÃ±adido al PATH.
- Tener `pip` actualizado:  
  ```bash
  python -m pip install --upgrade pip
  ```
- Acceso local a **LLaMA 3.2** (previamente autorizado por Meta y descargado vÃ­a Hugging Face).
- InstalaciÃ³n de **Git** (opcional pero recomendado).
- Uso de una terminal como PowerShell, Terminal de VSCode o Git Bash.

---

## ğŸ CreaciÃ³n del entorno virtual (Windows)

Sigue estos pasos para configurar el entorno virtual en Windows:

```plaintext
# setup_venv_windows.txt

1. Abre PowerShell o Git Bash en la carpeta raÃ­z del proyecto.

2. Ejecuta el siguiente comando para crear un entorno virtual:
   python -m venv venv

3. Activa el entorno virtual:
   .\venv\Scripts\activate

4. Instala todas las dependencias necesarias:
   pip install -r requirements.txt

```

Este proceso crea un entorno limpio y aislado, evitando conflictos con otras instalaciones de Python.

---

## ğŸ“¦ `requirements.txt`

El archivo incluye dependencias esenciales como:

```txt
flask                        
faiss-cpu        
langchain           
langchain-community        
langchain-huggingface     
transformers                  
sentence-transformers         
torch                           
numpy                   
PyMuPDF                         
requests                        
accelerate                      
python-dotenv
```

---

## ğŸš€ EjecuciÃ³n del sistema

### 1. Extraer y vectorizar documentos

Primero, extrae el texto de los PDFs y guÃ¡rdalo en formato `.txt`:

```bash
python data_extractor.py
```

Luego genera los embeddings y crea el Ã­ndice FAISS:

```bash
python vectorizer.py
```

Esto gestionarÃ¡ `txtdata/` contruyendo el Ã­ndice en `vector_db/`.

---

### 2. Levantar el servidor Flask con LLaMA

Abre una terminal nueva:

```bash
python llama_server_local.py
```

Esto iniciarÃ¡ el backend en `http://127.0.0.1:5666/`, que espera consultas del cliente y responde con texto generado por el modelo LLaMA 3.2.

---

### 3. Ejecutar la interfaz grÃ¡fica del cliente

En otra terminal (manteniendo el servidor encendido):

```bash
python llama_client_local.py
```

Se abrirÃ¡ una ventana grÃ¡fica que permite introducir preguntas. El sistema recuperarÃ¡ contexto relevante y generarÃ¡ respuestas usando el modelo.

---

## ğŸ“¥ Â¿CÃ³mo obtener acceso a LLaMA 3.2?

1. Accede a la pÃ¡gina oficial del modelo en Hugging Face:  
   ğŸ‘‰ [https://huggingface.co/meta-llama](https://huggingface.co/meta-llama)

2. Rellena el formulario de solicitud de Meta:
   - Usa un email institucional si es posible.
   - Describe tu propÃ³sito (por ejemplo, "TFG sobre bÃºsqueda con IA usando RAG").
   - Acepta los tÃ©rminos de licencia.

3. Una vez aprobado, podrÃ¡s descargarlo con `transformers`:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-3B")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-3B")
```

---

## ğŸ§  Â¿CÃ³mo funciona el sistema?

1. **ExtracciÃ³n**: El texto se extrae de PDFs y se limpia.
2. **VectorizaciÃ³n**: Se genera una base de vectores (embeddings) del texto con SentenceTransformers.
3. **RecuperaciÃ³n**: Ante una consulta, se buscan los vectores mÃ¡s cercanos en FAISS.
4. **GeneraciÃ³n**: Se construye un prompt con el contexto recuperado y se genera una respuesta utilizando el modelo LLaMA 3.2.

---

## ğŸ” Notas importantes

- AsegÃºrate de tener suficiente memoria (RAM/GPU) para LLaMA 3.2. Para CPU, puede ser mÃ¡s lento.
- Toda la informaciÃ³n sensible (claves, rutas a modelos) debe mantenerse fuera del cÃ³digo fuente pÃºblico.
- Este sistema es para **uso acadÃ©mico o personal**. El uso comercial requiere autorizaciÃ³n explÃ­cita de Meta AI.

---

## ğŸ¤ Instituciones involucradas

Este proyecto forma parte de un **Trabajo de Fin de Grado (TFG)** en IngenierÃ­a TelemÃ¡tica â€“ Universidad de AlcalÃ¡.  

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ sujeto a las licencias de uso personal y acadÃ©mico del modelo LLaMA de Meta AI.  
No redistribuyas el modelo ni lo uses con fines comerciales sin autorizaciÃ³n.

---

**Autor**: HÃ©ctor NÃºÃ±ez Calero 
**AÃ±o**: 2025/2026 s
**Contacto**: *[hector.nunez@edu.uah.es]*
