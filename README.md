# ğŸ§  Sistema de GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) con LLaMA 3.2 como asistente para consultas

Este repositorio contiene un sistema completo para la integraciÃ³n local de un modelo **LLaMA 3.2** (versiÃ³n 3B) en un entorno **RAG (Retrieval-Augmented Generation)**. El objetivo es permitir la bÃºsqueda y consulta sobre documentos locales mediante IA generativa y recuperaciÃ³n contextual.

---

## ğŸ“ Estructura del Proyecto (componentes esenciales)

```
ğŸ“¦ main_dir/
â”œâ”€â”€ setup_venv_windows.txt   # Instrucciones para configurar el entorno virtual en Windows
â”œâ”€â”€ requirements.txt         # Dependencias necesarias para el entorno virtual
â”œâ”€â”€ pdfdata/                 # Carpeta con PDFs originales
â”œâ”€â”€ txtdata/                 # Carpeta con texto plano (postprocesado)
â”œâ”€â”€ data_extractor.py        # ExtracciÃ³n de texto desde PDF a TXT
â”œâ”€â”€ vectorizer.py            # VectorizaciÃ³n, embeddings y base de datos FAISS
â”œâ”€â”€ local_client.py          # Cliente con GUI para consultas al servidor
â”œâ”€â”€ local_server.py          # Servidor Flask que aloja la instancia LLaMA 3.2 como LLM para las respuestas
â”œâ”€â”€ vector_db/               # Base de datos vectorial generada con FAISS
â””â”€â”€ README.md                # Archivo README del proyecto
```

---

## âš™ï¸ Requisitos previos

Antes de comenzar, asegÃºrese de cumplir con lo siguiente:

- Sistema operativo: **Windows 10/11** (con soporte para Python).
- **Python 3.10 o 3.11** instalado y **aÃ±adido al PATH**.
- Tener `pip` actualizado:  
  ```bash
  python -m pip install --upgrade pip
  ```
- Acceso local a **LLaMA 3.2** (previamente autorizado por Meta y descargado vÃ­a Hugging Face).
- Uso de una terminal (preferiblemente PowerShell)

---

## ğŸ“¦ `requirements.txt`

Se necesitan las siguientes dependencias externas a python:

```txt
flask                        
faiss-cpu        
langchain           
langchain-community        
langchain-huggingface     
transformers                  
sentence-transformers         
torch                           
numpy                   
PyMuPDF                         
requests                        
accelerate                      
googletrans
```

---

## ğŸ CreaciÃ³n del entorno virtual (Windows)

Siga estos pasos para configurar el entorno virtual en Windows:

```plaintext
# setup_venv_windows.txt

1. Abra la terminal de PowerShell.

2. Navegue al directorio del proyecto:
   cd ruta\al\directorio\del\proyecto

3. Cree un entorno virtual con:
   python -m venv venv

4. Active el entorno virtual mediante:
   .\venv\Scripts\activate

5. Instale todas las dependencias necesarias usando:
   pip install -r requirements.txt

```

Este proceso construye un entorno limpio y aislado, evitando conflictos con otras instalaciones de Python.

---

## ğŸš€ EjecuciÃ³n del sistema

### 1. Extraer y vectorizar documentos

En primer lugar, se extrae el texto de los archivos PDF del directorio pdfdata y guarda el contenido en formato .txt en el directorio txtdata.

```bash
python data_extractor.py
```

En segundo lugar, se divide los textos en chunks, genera vectores para cada chunk utilizando un modelo de embeddings y almacena los vectores en una base de datos vectorial (FAISS).

```bash
python vectorizer.py
```

Esto gestionarÃ¡ `txtdata/`, construyendo los objetos 'document', los vectores semÃ¡nticos y los Ã­ndices document-vector en `vector_db/`.

---

### 2. Levantar el servidor Flask con LLaMA 3.2

Se ejecuta el servidor Flask para responder a las consultas del cliente.

```bash
python llama_server_local.py
```

Esto iniciarÃ¡ el backend en `http://192.168.XX.XX:5666/`, que espera consultas del cliente para responder con texto generado por el modelo de LLM: LLaMA 3.2 3B.

---

### 3. Ejecutar la interfaz grÃ¡fica del cliente

Manteniendo el servidor encendido, se ejecuta el script cliente:

```bash
python llama_client_local.py
```

Se abrirÃ¡ una ventana grÃ¡fica que permite introducir preguntas. El sistema recuperarÃ¡ contexto relevante y generarÃ¡ respuestas usando el modelo.
Este programa utiliza la base de datos FAISS construida anteriormente para realizar recuperaciÃ³n de contexto a la hora de enviar el prompt para contestar las queries. 

---

## ğŸ Resumen del funcionamiento de este sistema RAG

1. **ExtracciÃ³n**: El texto se extrae de PDFs y se limpia.
2. **VectorizaciÃ³n**: Se genera una base de vectores semÃ¡nticos del texto.
3. **RecuperaciÃ³n**: Ante una consulta, se buscan los vectores mÃ¡s cercanos en FAISS.
4. **GeneraciÃ³n**: Se construye un prompt con el contexto recuperado, se solicita la consulta y se genera una respuesta utilizando el modelo LLaMA 3.2.

---

## ğŸ¤ JustificaciÃ³n del proyecto

Este proyecto es aprovechado para un **Trabajo de Fin de Grado (TFG)** en IngenierÃ­a TelemÃ¡tica â€“ Universidad de AlcalÃ¡.  

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ sujeto a las licencias de uso personal y acadÃ©mico del modelo LLaMA de Meta AI.  
No redistribuya el modelo ni lo uses con fines comerciales sin autorizaciÃ³n.

---

**Autor**: HÃ©ctor NÃºÃ±ez Calero.

**AÃ±o**: 2025/2026.

**Contacto**: *[hector.nunez@edu.uah.es]*.
