# üß† Sistema de Generaci√≥n por Recuperaci√≥n Aumentada (RAG) con LLaMA 3.2 como asistente para consultas

Este repositorio contiene un sistema completo para la integraci√≥n local de un modelo **LLaMA 3.2** (versi√≥n 3B) en un entorno **RAG (Retrieval-Augmented Generation)**. El objetivo es permitir la b√∫squeda y consulta sobre documentos locales mediante IA generativa y recuperaci√≥n contextual.

---

## üìÅ Estructura del Proyecto (componentes esenciales)

```
üì¶ main_dir/
‚îú‚îÄ‚îÄ setup_venv_windows.txt   # Instrucciones para configurar el entorno virtual en Windows
‚îú‚îÄ‚îÄ requirements.txt         # Dependencias necesarias para el entorno virtual
‚îú‚îÄ‚îÄ pdfdata/                 # Carpeta con PDFs originales
‚îú‚îÄ‚îÄ txtdata/                 # Carpeta con texto plano (postprocesado)
‚îú‚îÄ‚îÄ data_extractor.py        # Extracci√≥n de texto desde PDF a TXT
‚îú‚îÄ‚îÄ vectorizer.py            # Vectorizaci√≥n, embeddings y base de datos FAISS
‚îú‚îÄ‚îÄ llama_server_local.py    # Servidor Flask que aloja LLaMA 3.2 y gestiona generaci√≥n de texto
‚îú‚îÄ‚îÄ llama_client_local.py    # Cliente para consultas y sesiones (Tkinter + requests)
‚îú‚îÄ‚îÄ vector_db/               # Base de datos vectorial generada con FAISS
‚îî‚îÄ‚îÄ README.md                # Archivo README del proyecto
```

---

## ‚öôÔ∏è Requisitos previos

Antes de comenzar, aseg√∫rese de cumplir con lo siguiente:

- Sistema operativo: **Windows 10/11** (con soporte para Python).
- **Python 3.10 o 3.11** instalado y a√±adido al PATH.
- Tener `pip` actualizado:  
  ```bash
  python -m pip install --upgrade pip
  ```
- Acceso local a **LLaMA 3.2** (previamente autorizado por Meta y descargado v√≠a Hugging Face).
- Uso de una terminal (preferiblemente PowerShell)

---

## üì¶ `requirements.txt`

El archivo incluye dependencias esenciales como:

```txt
flask                        
faiss-cpu        
langchain           
langchain-community        
langchain-huggingface     
transformers                  
sentence-transformers         
torch                           
numpy                   
PyMuPDF                         
requests                        
accelerate                      
python-dotenv
```

---

## üêç Creaci√≥n del entorno virtual (Windows)

Siga estos pasos para configurar el entorno virtual en Windows:

```plaintext
# setup_venv_windows.txt

1. Abra la terminal de PowerShell.

2. Navegue al directorio del proyecto:
   cd ruta\al\directorio\del\proyecto

3. Cree un entorno virtual con:
   python -m venv venv

4. Active el entorno virtual mediante:
   .\venv\Scripts\activate

5. Instale todas las dependencias necesarias usando:
   pip install -r requirements.txt

```

Este proceso construye un entorno limpio y aislado, evitando conflictos con otras instalaciones de Python.

---

## üöÄ Ejecuci√≥n del sistema

### 1. Extraer y vectorizar documentos

En primer lugar, se extrae el texto de los archivos PDF del directorio pdfdata y guarda el contenido en formato .txt en el directorio txtdata.

```bash
python data_extractor.py
```

En segundo lugar, se divide los textos en chunks, genera vectores para cada chunk utilizando un modelo de embeddings y almacena los vectores en una base de datos FAISS.

```bash
python vectorizer.py
```

Esto gestionar√° `txtdata/` contruyendo los objetos 'document', vectores e √≠ndice en `vector_db/`.

---

### 2. Levantar el servidor Flask con LLaMA

Abra una terminal nueva:

```bash
python llama_server_local.py
```

Esto iniciar√° el backend en `http://127.0.0.1:5666/`, que espera consultas del cliente y responde con texto generado por el modelo LLaMA 3.2.

---

### 3. Ejecutar la interfaz gr√°fica del cliente

En otra terminal (manteniendo el servidor encendido):

```bash
python llama_client_local.py
```

Se abrir√° una ventana gr√°fica que permite introducir preguntas. El sistema recuperar√° contexto relevante y generar√° respuestas usando el modelo.
Utilice la base de datos FAISS construida anteriormente para realizar recuperaci√≥n de contexto a la hora de enviar el prompt con la query. 

---

## üì• C√≥mo se obtiene el acceso a LLaMA 3.2

1. Acceda a la p√°gina oficial del modelo en Hugging Face:  
    [https://huggingface.co/meta-llama](https://huggingface.co/meta-llama)

2. Rellene el formulario de solicitud de Meta:
   - Use un email institucional si es posible.
   - Describa tu prop√≥sito (por ejemplo, "TFG sobre b√∫squeda con IA usando RAG").
   - Acepte los t√©rminos de licencia.

3. Una vez aprobado, se descargar√° con `transformers`:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-3B")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-3B")
```

---

## üß† Funcionamiento resumido del sistema RAG

1. **Extracci√≥n**: El texto se extrae de PDFs y se limpia.
2. **Vectorizaci√≥n**: Se genera una base de vectores (embeddings) del texto.
3. **Recuperaci√≥n**: Ante una consulta, se buscan los vectores m√°s cercanos en FAISS.
4. **Generaci√≥n**: Se construye un prompt con el contexto recuperado y se genera una respuesta utilizando el modelo LLaMA 3.2.

---

## üîê Notas importantes

- Aseg√∫rese de tener suficiente memoria (RAM/GPU) para LLaMA 3.2. Para CPU, puede ser m√°s lento.
- Toda la informaci√≥n sensible (claves, rutas a modelos) debe mantenerse fuera del c√≥digo fuente p√∫blico.
- Este sistema es para **uso acad√©mico o personal**. El uso comercial requiere autorizaci√≥n expl√≠cita de Meta AI.

---

## ü§ù Justificaci√≥n del proyecto

Este proyecto forma parte de un **Trabajo de Fin de Grado (TFG)** en Ingenier√≠a Telem√°tica ‚Äì Universidad de Alcal√°.  

---

## üìÑ Licencia

Este proyecto est√° sujeto a las licencias de uso personal y acad√©mico del modelo LLaMA de Meta AI.  
No redistribuya el modelo ni lo uses con fines comerciales sin autorizaci√≥n.

---

**Autor**: H√©ctor N√∫√±ez Calero.

**A√±o**: 2025/2026.

**Contacto**: *[hector.nunez@edu.uah.es]*.
